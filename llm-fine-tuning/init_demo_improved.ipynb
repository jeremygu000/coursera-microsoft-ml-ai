{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc3c696",
   "metadata": {},
   "source": [
    "#### Step 1: Prepare the dataset\n",
    "##### Dataset collection\n",
    "Collect a dataset of anonymized patient feedback categorized by sentimentâ€”positive, neutral, and negative. Preprocessing includes cleaning, tokenizing, and splitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cef64e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        cleaned_text  label  \\\n",
      "0  the staff was very kind and attentive to my needs      0   \n",
      "1  the waiting time was too long and the staff wa...      2   \n",
      "2  the doctor answered all my questionsbut the fa...      1   \n",
      "3  the nurse was compassionate made me feel comfo...      0   \n",
      "4  i had to wait over an hour before being seen u...      2   \n",
      "\n",
      "                                    padded_tokenized  \n",
      "0  [101, 1996, 3095, 2001, 2200, 2785, 1998, 2012...  \n",
      "1  [101, 1996, 3403, 2051, 2001, 2205, 2146, 1998...  \n",
      "2  [101, 1996, 3460, 4660, 2035, 2026, 3980, 8569...  \n",
      "3  [101, 1996, 6821, 2001, 29353, 2081, 2033, 251...  \n",
      "4  [101, 1045, 2018, 2000, 3524, 2058, 2019, 3178...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the pre-trained BERT tokenizer for \"bert-base-uncased\" model\n",
    "# This tokenizer converts text into tokens that the BERT model can understand,\n",
    "# including handling special tokens, word piece tokenization, and vocabulary mapping\n",
    "# Note: The generated tokens are specific to BERT's vocabulary and tokenization rules,\n",
    "# so they're primarily designed for BERT-based models, though some other transformer models\n",
    "# may use similar tokenization schemes\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Create a noisy dataset\n",
    "data_dict = {\n",
    "    \"text\": [\n",
    "        \"  The staff was very kind and attentive to my needs!!!  \",\n",
    "        \"The waiting time was too long, and the staff was rude. Visit us at http://hospitalreviews.com\",\n",
    "        \"The doctor answered all my questions...but the facility was outdated.   \",\n",
    "        \"The nurse was compassionate & made me feel comfortable!! :) \",\n",
    "        \"I had to wait over an hour before being seen.  Unacceptable service! #frustrated\",\n",
    "        \"The check-in process was smooth, but the doctor seemed rushed. Visit https://feedback.com\",\n",
    "        \"Everyone I interacted with was professional and helpful.  \"\n",
    "    ],\n",
    "    \"label\": [\"positive\", \"negative\", \"neutral\", \"positive\", \"negative\", \"neutral\", \"positive\"]\n",
    "}\n",
    "\n",
    "# Convert dataset to a DataFrame\n",
    "data = pd.DataFrame(data_dict)\n",
    "\n",
    "# Clean the text\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "data[\"cleaned_text\"] = data[\"text\"].apply(clean_text)\n",
    "\n",
    "# Convert labels to integers\n",
    "label_map = {\"positive\": 0, \"neutral\": 1, \"negative\": 2}\n",
    "data[\"label\"] = data[\"label\"].map(label_map)\n",
    "\n",
    "# Tokenize the cleaned text\n",
    "# add_special_tokens=True: Tokenize text and add special tokens (CLS(101), SEP(102)) for model compatibility\n",
    "data['tokenized'] = data['cleaned_text'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "\n",
    "# Pad or truncate to fixed length (e.g., 128 tokens)\n",
    "data['padded_tokenized'] = data['tokenized'].apply(\n",
    "    lambda x: x + [tokenizer.pad_token_id] * (128 - len(x)) if len(x) < 128 else x[:128]\n",
    ")\n",
    "\n",
    "# Preview cleaned and labeled data\n",
    "print(data[['cleaned_text', 'label', 'padded_tokenized']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coursera-microsoft-ml-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
