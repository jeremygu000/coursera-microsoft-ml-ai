# ================================================================
#  BERT Fine-Tuning Example for Sentiment Classification
#  Using Hugging Face Transformers and Datasets
#  ------------------------------------------------
#  This version replaces manual padding/truncation with built-in
#  tokenizer features, adds detailed explanations, and uses
#  best practices for training small models on custom data.
# ================================================================

import pandas as pd
import re
from transformers import AutoTokenizer

# ------------------------------------------------
# 1Ô∏è‚É£ Load the tokenizer
# ------------------------------------------------
# 'bert-base-uncased' is a lowercase English BERT model.
# The tokenizer handles wordpiece tokenization and adds
# necessary special tokens ([CLS], [SEP], [PAD]) automatically.
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# ------------------------------------------------
# 2Ô∏è‚É£ Create a small example dataset
# ------------------------------------------------
data_dict = {
    "text": [
        "  The staff was very kind and attentive to my needs!!!  ",
        "The waiting time was too long, and the staff was rude. Visit us at http://hospitalreviews.com",
        "The doctor answered all my questions...but the facility was outdated.   ",
        "The nurse was compassionate & made me feel comfortable!! :) ",
        "I had to wait over an hour before being seen.  Unacceptable service! #frustrated",
        "The check-in process was smooth, but the doctor seemed rushed. Visit https://feedback.com",
        "Everyone I interacted with was professional and helpful.  "
    ],
    "label": ["positive", "negative", "neutral", "positive", "negative", "neutral", "positive"]
}

data = pd.DataFrame(data_dict)

# ------------------------------------------------
# 3Ô∏è‚É£ Clean the raw text
# ------------------------------------------------
# Remove URLs, punctuation, and excessive whitespace.
def clean_text(text):
    text = text.lower()  # normalize to lowercase
    text = re.sub(r'http\S+', '', text)  # remove URLs
    text = re.sub(r'[^\w\s]', '', text)  # remove punctuation
    text = re.sub(r'\s+', ' ', text).strip()  # normalize spaces
    return text

data["cleaned_text"] = data["text"].apply(clean_text)

# ------------------------------------------------
# 4Ô∏è‚É£ Convert labels to numeric values
# ------------------------------------------------
label_map = {"positive": 0, "neutral": 1, "negative": 2}
data["label"] = data["label"].map(label_map)

# ------------------------------------------------
# 5Ô∏è‚É£ Split dataset into train / validation / test sets
# ------------------------------------------------
from sklearn.model_selection import train_test_split

train_data, temp_data = train_test_split(data, test_size=0.3, random_state=42)
val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)

print(f"Training Size: {len(train_data)}, Validation Size: {len(val_data)}, Test Size: {len(test_data)}")

# ------------------------------------------------
# 6Ô∏è‚É£ Convert pandas DataFrames to Hugging Face Datasets
# ------------------------------------------------
from datasets import Dataset

train_dataset = Dataset.from_pandas(train_data)
val_dataset = Dataset.from_pandas(val_data)
test_dataset = Dataset.from_pandas(test_data)

# ------------------------------------------------
# 7Ô∏è‚É£ Tokenize text using the tokenizer
# ------------------------------------------------
# Instead of manually padding or truncating,
# we use the tokenizer's built-in functionality.
# This automatically adds:
#   - [CLS] at the start
#   - [SEP] at the end
#   - [PAD] up to the max_length if needed
#   - truncation beyond the max_length
# It also generates an `attention_mask` that tells the model
# which tokens are padding and should be ignored.
def tokenize_function(examples):
    return tokenizer(
        examples["cleaned_text"],
        padding="max_length",     # pad all sequences to the same length
        truncation=True,          # truncate sequences longer than max_length
        max_length=128            # set maximum input length (adjustable)
    )

train_dataset = train_dataset.map(tokenize_function, batched=True)
val_dataset = val_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

# ------------------------------------------------
# 8Ô∏è‚É£ Remove unnecessary columns and format datasets
# ------------------------------------------------
# Keep only the model inputs: input_ids, attention_mask, label, which are generated by tokenize_function
train_dataset = train_dataset.remove_columns(["text", "cleaned_text"])
val_dataset = val_dataset.remove_columns(["text", "cleaned_text"])
test_dataset = test_dataset.remove_columns(["text", "cleaned_text"])

train_dataset = train_dataset.with_format("torch")
val_dataset = val_dataset.with_format("torch")
test_dataset = test_dataset.with_format("torch")

# ------------------------------------------------
# 9Ô∏è‚É£ Load a pre-trained BERT model for classification
# ------------------------------------------------
from transformers import AutoModelForSequenceClassification

# We specify num_labels=3 since we have three sentiment classes.
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=3
)

# ------------------------------------------------
# üîü Define training arguments
# ------------------------------------------------
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",             # output directory for checkpoints
    learning_rate=2e-5,                 # 0.00002Ôºå standard fine-tuning LR for BERT
    per_device_train_batch_size=16,
    num_train_epochs=3,
    eval_strategy="epoch",        # evaluate at the end of each epoch
    logging_strategy="epoch",           # log results after each epoch
    save_strategy="epoch",              # save best model each epoch
    load_best_model_at_end=True,        # reload best model when training ends
    logging_dir="./logs"                # directory for logs
)

# ------------------------------------------------
# 11Ô∏è‚É£ Create Trainer and define metrics
# ------------------------------------------------
from transformers import Trainer
from sklearn.metrics import accuracy_score, f1_score
import numpy as np

# Compute accuracy and F1 for each evaluation
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1": f1_score(labels, preds, average="weighted"),
    }

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# ------------------------------------------------
# 12Ô∏è‚É£ Start training
# ------------------------------------------------
trainer.train()

# ------------------------------------------------
# 13Ô∏è‚É£ Evaluate on the test set
# ------------------------------------------------
test_results = trainer.evaluate(test_dataset)
print(f"Test Accuracy: {test_results['eval_accuracy']:.4f}, F1 Score: {test_results['eval_f1']:.4f}")

# ------------------------------------------------
# 14Ô∏è‚É£ Save model and tokenizer
# ------------------------------------------------
model.save_pretrained("./fine_tuned_bert")
tokenizer.save_pretrained("./fine_tuned_bert")

print("‚úÖ Model and tokenizer saved successfully!")
