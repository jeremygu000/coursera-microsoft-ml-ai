{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41c2eb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "121a7ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset: Study hours, previous exam scores, and pass/fail labels\n",
    "data = {\n",
    "    'StudyHours': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'PrevExamScore': [30, 40, 45, 50, 60, 65, 70, 75, 80, 85],\n",
    "    'Pass': [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]  # 0 = Fail, 1 = Pass\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Features and target variable\n",
    "X = df[['StudyHours', 'PrevExamScore']]\n",
    "y = df['Pass']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efb3be1",
   "metadata": {},
   "source": [
    "### LASSO (without feature standardization)\n",
    "Coefficients are not directly comparable across features with different scales; the L1 penalty becomes unfair and may zero out the “smaller-scale” features. Results and feature selection can be unstable. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24e8ff1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Coefficients: [0.08153909 0.01180619]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Lasso model with alpha (λ) as the regularization parameter\n",
    "lasso_model = Lasso(alpha=0.01)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Display the coefficients of the features\n",
    "print(f\"Lasso Coefficients: {lasso_model.coef_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d068847",
   "metadata": {},
   "source": [
    "### LASSO (with feature standardization)\n",
    "All features are scaled to mean=0 and std=1, making the L1 penalty fair and coefficients comparable (per 1 SD). Feature selection becomes more meaningful and CV/hyperparameter tuning more stable. Use a Pipeline to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c428e7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha (via CV): 0.1\n",
      "Selected (non-zero) features: 1 / 2\n",
      "         Feature  Lasso_Coefficient\n",
      "0     StudyHours           0.317815\n",
      "1  PrevExamScore           0.000000\n",
      "CV R^2 per fold: [0.32578012 0.78164702 0.2458622         nan        nan]\n",
      "Mean CV R^2: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1283: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1283: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# L1 regularization is sensitive to feature scale.\n",
    "# Use a Pipeline so the scaler is fit ONLY on the training folds (no data leakage).\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),                 # mean=0, std=1 for each feature\n",
    "    (\"lasso\", LassoCV(alphas=[0.001, 0.01, 0.1, 1],\n",
    "                      cv=5, max_iter=5000, random_state=42))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "best_alpha = pipe.named_steps[\"lasso\"].alpha_\n",
    "coefs = pipe.named_steps[\"lasso\"].coef_\n",
    "\n",
    "print(\"Best alpha (via CV):\", best_alpha)\n",
    "print(\"Selected (non-zero) features:\", (coefs != 0).sum(), \"/\", len(coefs))\n",
    "\n",
    "# Map coefficients back to feature names for readability\n",
    "coef_df = pd.DataFrame({\"Feature\": X_train.columns, \"Lasso_Coefficient\": coefs})\n",
    "print(coef_df.sort_values(\"Lasso_Coefficient\", ascending=False).reset_index(drop=True))\n",
    "\n",
    "# Optional: evaluate with CV on the whole pipeline (scaler refit inside each fold)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5, scoring=\"r2\")\n",
    "print(\"CV R^2 per fold:\", cv_scores)\n",
    "print(\"Mean CV R^2:\", np.mean(cv_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb6545a",
   "metadata": {},
   "source": [
    "## Ridge (without feature standardization)\n",
    "\n",
    "**What this block does**\n",
    "- Fits a `Ridge(alpha=1.0)` model on raw (unscaled) features.\n",
    "- Prints raw coefficients and intercept.\n",
    "- Reports cross-validated \\(R^2\\) with `cv=4`.\n",
    "\n",
    "**Why results can be misleading**\n",
    "- **Different feature scales** (e.g., `StudyHours: 0–10` vs `PrevExamScore: 0–100`) make raw coefficients **not directly comparable**.\n",
    "- The L2 penalty acts on the **magnitude of coefficients**. Small-scale features typically need **larger numeric coefficients** to have a similar effect on \\(y\\), so they get **penalized more**, which can distort interpretability.\n",
    "- CV scores can be more **variable/unstable** when scale differs across features.\n",
    "\n",
    "**How to read the output**\n",
    "- `Coefficients`: effects **per 1 unit** in the *original units* (not comparable across features of different scales).\n",
    "- `Intercept`: baseline prediction when all features are zero (in original units).\n",
    "- `Cross-validated R^2`: average out-of-fold fit; treat cautiously when inputs are unscaled.\n",
    "\n",
    "**Takeaway**\n",
    "> Use this block only for demonstration. For fair regularization and interpretable coefficients, prefer the standardized pipeline below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b9fa69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.07407407 0.01304121]\n",
      "Intercept: -0.6898800208659339\n",
      "Cross-validated R^2: 0.3662920165248332\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# NOTE: No standardization here.\n",
    "# If features have different scales (e.g., StudyHours: 0–10 vs PrevExamScore: 0–100),\n",
    "# the raw coefficients are NOT directly comparable.\n",
    "# L2 penalty in Ridge acts on |beta|, so a small-scale feature tends to get a larger\n",
    "# numerical coefficient, which is penalized more—even if its \"true\" effect is similar.\n",
    "# This can distort interpretability and make regularization \"unfair\" across features.\n",
    "\n",
    "ridge = Ridge(alpha=1.0)  # alpha = λ\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "print(\"Coefficients:\", ridge.coef_) # L2 regularization coefficient\n",
    "print(\"Intercept:\", ridge.intercept_) # L2 regularization intercept\n",
    "\n",
    "score = np.mean(cross_val_score(ridge, X_train, y_train, cv=4)) # cv=3 because not enough samples\n",
    "print(\"Cross-validated R^2:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ddfb51",
   "metadata": {},
   "source": [
    "## Ridge (with feature standardization via Pipeline + RidgeCV)\n",
    "\n",
    "**What this block does**\n",
    "- Builds a `Pipeline` with `StandardScaler` (mean=0, std=1) and `RidgeCV` to **select alpha via CV**.\n",
    "- Fits the pipeline, prints the **best alpha**, standardized **coefficients**, a tidy coefficient table, and **CV \\(R^2\\)** per fold.\n",
    "\n",
    "**Why standardization matters**\n",
    "- Puts all features on the **same scale**, so L2 penalty treats coefficients **evenly** (fair regularization).\n",
    "- Coefficients become **comparable**: each reflects the change in \\(y\\) (in SD units) per **1 SD** increase in the feature.\n",
    "- `Pipeline` avoids **data leakage**: the scaler is fit **inside each fold** during CV.\n",
    "- Typically yields **more stable** CV/hyperparameter selection and better generalization.\n",
    "\n",
    "**How to read the output**\n",
    "- `Best alpha (via CV)`: regularization strength chosen to balance bias–variance.\n",
    "- `Standardized coefficients`: effects **per 1 SD** of each feature (now comparable across features).\n",
    "- `CV R^2 per fold` & `Mean CV R^2`: out-of-fold performance; inspect spread to gauge stability.\n",
    "\n",
    "**Extra notes**\n",
    "- With **highly correlated features**, Ridge tends to **share weight** across them (no coefficients exactly zero).\n",
    "- For automatic feature elimination, consider **LASSO** or **Elastic Net** (with nonzero L1 component).\n",
    "\n",
    "**Takeaway**\n",
    "> Standardization + Pipeline + RidgeCV = fair penalties, comparable coefficients, and reliable model selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12af2f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha (via CV): 10.0\n",
      "Standardized coefficients: {'StudyHours': np.float64(0.12959298903600444), 'PrevExamScore': np.float64(0.1270166191709816)}\n",
      "         Feature  Std_Coefficient\n",
      "0     StudyHours         0.129593\n",
      "1  PrevExamScore         0.127017\n",
      "CV R^2 per fold: [-0.03476487  0.91030999  0.02135171  0.66114942]\n",
      "Mean CV R^2: 0.3895115658031173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1283: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1283: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1283: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1283: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1283: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1283: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1135: UserWarning: One or more of the test scores are non-finite: [nan nan nan]\n",
      "  warnings.warn(\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1283: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1283: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1283: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1283: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1283: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1283: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1135: UserWarning: One or more of the test scores are non-finite: [nan nan nan]\n",
      "  warnings.warn(\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1283: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1283: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1283: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1283: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1283: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1283: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1135: UserWarning: One or more of the test scores are non-finite: [nan nan nan]\n",
      "  warnings.warn(\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1283: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1283: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1283: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1283: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1283: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1283: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1135: UserWarning: One or more of the test scores are non-finite: [nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Standardization puts all features on the same scale (mean=0, std=1).\n",
    "# Benefits:\n",
    "# - Fair regularization: L2 penalty treats all coefficients evenly.\n",
    "# - Coefficient comparability: each coefficient ≈ effect per 1 SD increase in the feature.\n",
    "# - More stable CV / hyperparameter selection.\n",
    "# Using a Pipeline ensures the scaler is fit ONLY on training folds (no data leakage).\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"ridge\", RidgeCV(alphas=[0.1, 1.0, 10.0], cv=4))  # CV selects the best alpha\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "best_alpha = pipe.named_steps[\"ridge\"].alpha_\n",
    "coefs = pipe.named_steps[\"ridge\"].coef_\n",
    "\n",
    "print(\"Best alpha (via CV):\", best_alpha)\n",
    "print(\"Standardized coefficients:\", dict(zip(X_train.columns, coefs)))\n",
    "# Interpretation: each coefficient is the change in y (in SD units) per 1 SD increase in that feature.\n",
    "\n",
    "# Optional: show coefficients neatly\n",
    "coef_df = pd.DataFrame({\"Feature\": X_train.columns, \"Std_Coefficient\": coefs})\n",
    "print(coef_df)\n",
    "\n",
    "# Evaluate the whole pipeline with CV (scaler is refit in each fold properly)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=4, scoring=\"r2\")\n",
    "print(\"CV R^2 per fold:\", cv_scores)\n",
    "print(\"Mean CV R^2:\", np.mean(cv_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a5e70a",
   "metadata": {},
   "source": [
    "### Why the coefficients look so different — and why standardization matters\n",
    "\n",
    "When features live on very different scales (e.g., **StudyHours: 0–10** vs **PrevExamScore: 0–100**), raw linear/Ridge coefficients are **not directly comparable**. A one-unit increase in StudyHours is a much bigger relative change than a one-unit increase in PrevExamScore, so the model assigns a **larger numeric coefficient** to StudyHours (e.g., 0.074 vs 0.013) even if their *true* effects are similar. This also makes L2 regularization **unfair**: Ridge penalizes the magnitude of coefficients, so small-scale features are effectively penalized more.\n",
    "\n",
    "Standardizing features to **mean = 0, std = 1** fixes this:\n",
    "- The L2 penalty treats all coefficients evenly (“fair regularization”).\n",
    "- Coefficients become **interpretable per 1 standard deviation** of each feature and thus comparable (e.g., both ≈0.13 after scaling).\n",
    "- Cross-validation and hyperparameter selection become more stable.\n",
    "- Using a `Pipeline` ensures the scaler is fit **inside each CV fold** (no data leakage).\n",
    "\n",
    "**Bottom line:** Standardize continuous features before Ridge/LASSO/Elastic Net to get fair penalties, comparable coefficients, and more reliable model selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099fc8d1",
   "metadata": {},
   "source": [
    "## Elastic Net (without feature standardization)\n",
    "\n",
    "**What this block does**\n",
    "- Fits `ElasticNetCV` directly on raw (unscaled) features.\n",
    "- Uses CV to select the best `alpha` (overall penalty) and `l1_ratio` (L1–L2 balance).\n",
    "- Prints intercept, coefficients, and a tidy coefficient table; also inspects feature correlations.\n",
    "\n",
    "**Why results can be misleading**\n",
    "- With features on **different scales** (e.g., 0–10 vs 0–100), the **L1/L2 penalties act unfairly** because they penalize the *numeric size* of coefficients, not the real effect size.\n",
    "- Coefficients across unscaled features are **not directly comparable**; feature selection (zeros from L1) can become **unstable**.\n",
    "- CV may prefer hyperparameters that compensate for scale, not true predictive value.\n",
    "\n",
    "**How to read the output**\n",
    "- `Best alpha` and `Best l1_ratio`: chosen by CV but **scale-dependent**.\n",
    "- `Coefficients`: effects **per 1 unit in original units**; not comparable across features with different magnitudes.\n",
    "- Correlation matrix helps explain **redundancy**; highly correlated features can cause one coefficient to be shrunk toward zero.\n",
    "\n",
    "**Takeaway**\n",
    "> Use this unscaled version only as a baseline demo. For fair penalties, stable selection, and interpretable coefficients, prefer the standardized pipeline below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "405e05f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 10.0\n",
      "Best l1_ratio: 0.1\n",
      "Intercept: -0.7411971830985915\n",
      "Coefficients: [0.         0.02068662]\n",
      "         Feature  Coefficient\n",
      "0     StudyHours     0.000000\n",
      "1  PrevExamScore     0.020687\n",
      "\n",
      "Feature correlation matrix:\n",
      "               StudyHours  PrevExamScore\n",
      "StudyHours       1.000000       0.993809\n",
      "PrevExamScore    0.993809       1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.580e-03, tolerance: 1.500e-04\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.841e-04, tolerance: 1.500e-04\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/isdance/Desktop/projects/coursera-microsoft-ml-ai/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.758e-04, tolerance: 1.500e-04\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Elastic Net model with built-in cross-validation\n",
    "# - l1_ratio controls the balance between L1 (LASSO) and L2 (Ridge) penalties\n",
    "# - alphas are the candidate regularization strengths (λ values)\n",
    "# - cv sets the number of cross-validation folds\n",
    "model = ElasticNetCV(\n",
    "    l1_ratio=[0.1, 0.5, 0.9],\n",
    "    alphas=[0.001, 0.01, 0.1, 1, 10],\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Display the best hyperparameters selected via cross-validation\n",
    "print(\"Best alpha:\", model.alpha_)       # Optimal regularization strength (λ)\n",
    "print(\"Best l1_ratio:\", model.l1_ratio_) # Optimal balance between L1 and L2 penalties\n",
    "\n",
    "# Display model parameters\n",
    "print(\"Intercept:\", model.intercept_)    # Intercept term (bias)\n",
    "print(\"Coefficients:\", model.coef_)      # Coefficients of the selected features\n",
    "\n",
    "# Create a readable summary of coefficients mapped to their corresponding features\n",
    "coef_df = pd.DataFrame({\n",
    "    \"Feature\": X_train.columns,\n",
    "    \"Coefficient\": model.coef_\n",
    "})\n",
    "print(coef_df)\n",
    "\n",
    "# Examine feature correlations — helps explain why some coefficients are zero\n",
    "print(\"\\nFeature correlation matrix:\")\n",
    "print(X_train.corr())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9e2f11",
   "metadata": {},
   "source": [
    "## Elastic Net (with feature standardization via Pipeline + ElasticNetCV)\n",
    "\n",
    "**What this block does**\n",
    "- Builds a `Pipeline` with `StandardScaler` (mean=0, std=1) and `ElasticNetCV`.\n",
    "- Performs CV *inside the pipeline* (the scaler is fit **within each fold**, avoiding data leakage).\n",
    "- Prints the best `alpha`, best `l1_ratio`, intercept, standardized coefficients, and a readable coefficient table.\n",
    "\n",
    "**Why standardization matters**\n",
    "- Puts all features on the **same scale**, making L1/L2 penalties **fair and comparable**.\n",
    "- Coefficients become **interpretable per 1 standard deviation** increase in each feature (now comparable across features).\n",
    "- Leads to **more stable** CV/hyperparameter selection and typically better generalization.\n",
    "- `Pipeline` ensures proper **train/test separation** for scaling during CV.\n",
    "\n",
    "**How to read the output**\n",
    "- `Best alpha` / `Best l1_ratio`: data-driven balance between sparsity (L1) and shrinkage (L2).\n",
    "- `Coefficients`: change in \\(y\\) (in its native units) per **1 SD** increase in each feature (since only \\(X\\) is standardized).\n",
    "- Intercept reflects the baseline when standardized features are 0 (i.e., at their mean levels), **not necessarily near 0**.\n",
    "\n",
    "**Extra notes**\n",
    "- If \\(X\\) is sparse, use `StandardScaler(with_mean=False)`.\n",
    "- Increase `max_iter` if you see convergence warnings.\n",
    "- With highly correlated features, Elastic Net (nonzero L2) tends to **share weight** more stably than pure LASSO.\n",
    "\n",
    "**Takeaway**\n",
    "> Standardization + Pipeline + ElasticNetCV = fair regularization, comparable coefficients, stable feature selection, and leakage-free model evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cd55c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.1\n",
      "Best l1_ratio: 0.5\n",
      "Intercept: 0.5\n",
      "Coefficients: [0.20814529 0.15021085]\n",
      "         Feature  Coefficient\n",
      "0     StudyHours     0.208145\n",
      "1  PrevExamScore     0.150211\n",
      "\n",
      "Feature correlation matrix:\n",
      "               StudyHours  PrevExamScore\n",
      "StudyHours       1.000000       0.993809\n",
      "PrevExamScore    0.993809       1.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Standardize features inside CV to ensure fair L1/L2 penalties and no data leakage\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"enet\", ElasticNetCV(\n",
    "        l1_ratio=[0.1, 0.5, 0.9],\n",
    "        alphas=[0.001, 0.01, 0.1, 1, 10],\n",
    "        cv=5,\n",
    "        max_iter=5000,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "enet = pipe.named_steps[\"enet\"]\n",
    "print(\"Best alpha:\", enet.alpha_)        # Optimal regularization strength (λ)\n",
    "print(\"Best l1_ratio:\", enet.l1_ratio_)  # Optimal L1/L2 balance\n",
    "print(\"Intercept:\", enet.intercept_)     # Note: after standardization, intercept often near 0\n",
    "print(\"Coefficients:\", enet.coef_)       # Comparable: effect per 1 SD increase in each feature\n",
    "\n",
    "coef_df = pd.DataFrame({\"Feature\": X_train.columns, \"Coefficient\": enet.coef_})\n",
    "print(coef_df)\n",
    "\n",
    "# (Optional) Inspect correlations to understand redundancy / why some coefficients are zero\n",
    "print(\"\\nFeature correlation matrix:\")\n",
    "print(X_train.corr())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a8b118",
   "metadata": {},
   "source": [
    "## Interpretation of the standardized Elastic Net results\n",
    "\n",
    "- **Best alpha = 0.1**  \n",
    "  Mild overall regularization; coefficients are shrunk but not heavily.\n",
    "\n",
    "- **Best l1_ratio = 0.5**  \n",
    "  Balanced Elastic Net (L1 + L2).  \n",
    "  → L1 gives some sparsity pressure, L2 stabilizes weights and shares them across correlated features.\n",
    "\n",
    "- **Intercept ≈ 0.5**  \n",
    "  With **X standardized (mean=0)**, the intercept is the model’s baseline prediction when features are at their means  \n",
    "  (≈ mean of *y* for regression, or the base positive rate if *y* is 0/1).\n",
    "\n",
    "- **Coefficients (comparable after scaling)**  \n",
    "  - `StudyHours ≈ 0.208`  \n",
    "  - `PrevExamScore ≈ 0.150`  \n",
    "  Interpretation: **+1 standard deviation** in a feature increases the prediction by the shown amount (in *y*’s units).  \n",
    "  Both are positive; **StudyHours is slightly stronger**.\n",
    "\n",
    "- **Feature correlation ≈ 0.994 (very high)**  \n",
    "  The two features carry almost the same information.  \n",
    "  With L2 in Elastic Net, the model **shares weight** between them rather than dropping one (as pure LASSO might).\n",
    "\n",
    "> **If you want a sparser model:** increase `l1_ratio` (e.g., 0.8–0.9) and/or `alpha`.  \n",
    "> **If you want stability with correlated features:** keep some L2 (current setting is reasonable).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coursera-microsoft-ml-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
